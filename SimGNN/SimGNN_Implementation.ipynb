{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimGNN-Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZvDqcbO9RlP",
        "outputId": "88bbbf5a-90a9-4bba-9f3c-c6aa585bd15c"
      },
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device, torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0 1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48iLmgjV9hG5",
        "outputId": "9b7cba37-9aad-4aa0-ca89-1be152e9121c"
      },
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-geometric\n",
        "import torch_geometric"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_scatter-2.0.8-cp37-cp37m-linux_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 3.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.8\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_sparse-0.6.11-cp37-cp37m-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.11\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-1.7.2.tar.gz (222 kB)\n",
            "\u001b[K     |████████████████████████████████| 222 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.5.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.0-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.7.2-py3-none-any.whl size=388142 sha256=da931ac5ee301e7caa8b9a65e35a1a42fadc6b4cf5f347d0832175ec1a2c39d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/93/b6/2eeb0465afe89aee74d7a07a606e9770466d7565abd45a99d5\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.0 torch-geometric-1.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbDAQ1Vz91ZM"
      },
      "source": [
        "# **1) Attention Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq7ZWsSc99EF"
      },
      "source": [
        "from torch_scatter import scatter_mean, scatter_add\n",
        "\n",
        "# Defining the Attention Mechanism as a Separate Class of itself\n",
        "class AttentionLayer(torch.nn.Module):\n",
        "    def __init__(self, d = 16, activation=2, a = 0.1):\n",
        "        \"\"\"\n",
        "        :param: d: Output Dimension of the Node Embeddings\n",
        "        :param: activation: The Activation Function to be used for the Attention Layer\n",
        "        :param: a: Slope of the -ve part if the activation is Leaky ReLU\n",
        "        \"\"\"\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.d = d # Output dimension of the Convolution Vector for each Node\n",
        "        self.activation = activation \n",
        "        self.a = a # Slope of the negative part in Leaky-ReLU\n",
        "        \n",
        "        self.params()\n",
        "        self.initialize()\n",
        "        \n",
        "    def params(self):\n",
        "        self.W_att = torch.nn.Parameter(torch.Tensor(self.d, self.d))\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        Initialization depends upon the activation function used.\n",
        "        If ReLU/ Leaky ReLU : He (Kaiming) Initialization\n",
        "        If tanh/ sigmoid : Xavier Initialization\n",
        "        0-ReLU, 1-Leaky ReLU, 2-tanh, 3-sigmoid\n",
        "        \"\"\"\n",
        "        non_lin = {0:\"relu\", 1:\"leaky_relu\", 2:\"tanh\", 3:\"sigmoid\"}\n",
        "        \n",
        "        if (self.activation==1) or (self.activation==0):\n",
        "            torch.nn.init.kaiming_normal_(self.W_att, a=self.a, nonlinearity=non_lin[self.activation])\n",
        "        elif (self.activation==2) or (self.activation==3):\n",
        "            torch.nn.init.xavier_normal_(self.W_att)\n",
        "        else:\n",
        "            raise ValueError(\"Activation can only take values 0,1,2,3!\")\n",
        "\n",
        "    def forward(self, node_embeddings, batch, size = None):\n",
        "        \"\"\" \n",
        "        :param: node_embeddings :(N_B x D) Tensor containing Node Embeddings\n",
        "        :param: batch : Tensor containing the Graph to which Each Node in the Batch belongs\n",
        "        :param: size : Check Documentation https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/mean.html\n",
        "        :return: global_graph_embedding for each graph in the batch\n",
        "        \"\"\"\n",
        "        size = batch[-1].item()+1 if size is None else size # Gives Batch Size = B\n",
        "        \n",
        "        global_context = scatter_mean(node_embeddings,index = batch, dim=0, dim_size = size) # (N_B,D) -> (B,D) (mean)\n",
        "        global_context = torch.matmul(global_context, self.W_att) # (B,D) x (D,D) -> (B,D)\n",
        "        \n",
        "        # Applying the Non-Linearity over W_att*mean(U_i), the default is tanh\n",
        "        if self.activation==2:\n",
        "            global_context = torch.tanh(global_context)\n",
        "        elif self.activation==1:\n",
        "            leaky_relu = torch.nn.LeakyReLU()\n",
        "            global_context = leaky_relu(global_context)\n",
        "        elif self.activation==0:\n",
        "            global_context = global_context.relu()\n",
        "        elif self.activation==3:\n",
        "            global_context = torch.sigmoid(global_context)\n",
        "        \n",
        "        # Getting the attention value for each Node for a Given Graph\n",
        "        e = torch.sum(node_embeddings*global_context[batch], dim=1) # (N_B,D) * (N_B,D) -> (N_B,1) (due to sum along dimension D) \n",
        "        attn_weights = e.sigmoid() # (N_B, 1)\n",
        "        \n",
        "        # Calculating the Global Graph Embedding\n",
        "        global_graph_embedding = scatter_add(node_embeddings*attn_weights.unsqueeze(-1),\n",
        "                                             index=batch, dim=0, dim_size=size) # (N_B,D) x (N_B,1) -> (B,D)\n",
        "        \n",
        "        return global_graph_embedding"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAGbUYcmAg5l"
      },
      "source": [
        "# **2) Neural Tensor Network Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l-BA5-0AlFV"
      },
      "source": [
        "# Defining the Neural Tensor Network Layer as a Separate Class of Itself\n",
        "class NTNLayer(torch.nn.Module):\n",
        "    def __init__(self, d=16, k=16, activation=0, a = 0.1):\n",
        "        \"\"\"\n",
        "        :param: d: Input Dimension of the NTN - i.e Dimension of the Graph/ Node Embeddings\n",
        "        :param: k: Output Dimension of the NTN - No. of Similarity Scores to output\n",
        "        :param: activation: Activation Function to be used for the NTN - Default = ReLU\n",
        "        :param: a: Slope of the negative part for a Leaky ReLU activation\n",
        "        \"\"\"\n",
        "        super(NTNLayer, self).__init__()\n",
        "        self.d = d # Input Dimension of the NTN\n",
        "        self.k = k # Output dimension of the NTN\n",
        "        self.a = a # Slope in case of Leaky ReLU initialization \n",
        "        self.activation = activation\n",
        "        self.params()\n",
        "        self.initialize()\n",
        "    \n",
        "    def params(self):\n",
        "        self.W = torch.nn.Parameter(torch.Tensor(self.d,self.d,self.k))\n",
        "        self.V = torch.nn.Parameter(torch.Tensor(self.k, 2*self.d))\n",
        "        self.b = torch.nn.Parameter(torch.Tensor(self.k,1))\n",
        "\n",
        "    def initialize(self): \n",
        "        \"\"\"\n",
        "        Initialization depends upon the activation function used.\n",
        "        If ReLU/ Leaky ReLU : He Initialization\n",
        "        If tanh/ sigmoid : Xavier Initialization\n",
        "        0-ReLU, 1-Leaky ReLU, 2-tanh, 3-sigmoid\n",
        "        \"\"\"\n",
        "        non_lin = {0:\"relu\", 1:\"leaky_relu\", 2:\"tanh\", 3:\"sigmoid\"}\n",
        "        if (self.activation==1) or (self.activation==0):\n",
        "            torch.nn.init.kaiming_normal_(self.W, a=self.a, nonlinearity=non_lin[self.activation])\n",
        "            torch.nn.init.kaiming_normal_(self.V, a=self.a, nonlinearity=non_lin[self.activation])\n",
        "            torch.nn.init.kaiming_normal_(self.b, a=self.a, nonlinearity=non_lin[self.activation])\n",
        "            \n",
        "        elif (self.activation==2) or (self.activation==3):\n",
        "            torch.nn.init.xavier_normal_(self.W)\n",
        "            torch.nn.init.xavier_normal_(self.V)\n",
        "            torch.nn.init.xavier_normal_(self.b)\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(\"Activation can only take values 0,1,2,3!\")\n",
        "        \n",
        "    def forward(self, h1, h2):\n",
        "        \"\"\"Returns 'K' Rough Similarity Scores between the Pair of Graphs\n",
        "        The Neural Tensor Network (NTN) outputs 'K' similarity scores where 'K' is a hyperparameter\n",
        "        :param: h1 : Graph Embedding of Graph 1 - (B,D)\n",
        "        :param: h2 : Graph Embedding of Graph 2 - (B,D)\n",
        "        \"\"\"\n",
        "        B,_ = h1.shape\n",
        "        scores = torch.mm(h1, self.W.view(self.d, -1)) # (B,D) x (D, K+D) -> (B, K+D)\n",
        "        scores = scores.view(B,self.d,self.k) # (B,K+D) -> (B,D,K)\n",
        "        scores = (scores*h2.unsqueeze(-1)).sum(dim=1) # (B,D,K) * (B,D,1) -> (B,K)\n",
        "        \n",
        "        concatenated_rep = torch.cat((h1, h2), dim=1) # (B,2D)\n",
        "        scores = scores + torch.mm(concatenated_rep, self.V.t()) # (B,2D) x (2D,K) -> (B,K)\n",
        "        scores = scores + self.b.t() # (B,K) + (1,K) = (B,K)\n",
        "        \n",
        "        if self.activation==0:\n",
        "            scores = scores.relu()\n",
        "            return scores\n",
        "        elif self.activation==1:\n",
        "            leaky_relu = torch.nn.LeakyReLU()\n",
        "            scores = leaky_relu(scores)\n",
        "            return scores\n",
        "        elif self.activation==2:\n",
        "            scores = torch.tanh(scores)\n",
        "            return scores\n",
        "        elif self.activation==3:\n",
        "            scores = torch.sigmoid(scores)\n",
        "            return scores\n",
        "        else:\n",
        "            raise ValueError(\"Activation can only take values 0,1,2,3!\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX8o_JZ7Cjkt"
      },
      "source": [
        "# **3) SimGNN - Putting it Together**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Zj-MX2CoK5"
      },
      "source": [
        "# Writing the Entire SimGNN Model\n",
        "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.nn import Linear\n",
        "\n",
        "class SimGNN(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, hist=True, ntn_layer=True, d=16, k=16, bins = 16):\n",
        "        super(SimGNN, self).__init__()\n",
        "        self.setupHyperParams(hist, ntn_layer, d, k, bins, num_node_features)\n",
        "        self.setupLayers()\n",
        "        \n",
        "    def setupHyperParams(self, hist, ntn_layer, d, k, bins, num_node_features):\n",
        "        self.num_node_features = num_node_features\n",
        "        # Dimension of the Node/ Graph Embeddding\n",
        "        self.d = d\n",
        "        # Output Dimension of the NTN\n",
        "        self.k = k\n",
        "        # Do we want to include the NTN Layer in the pipeline\n",
        "        self.ntn_layer = ntn_layer\n",
        "        # Do we want to use the histogram strategy\n",
        "        self.hist = hist\n",
        "        # No. of Bins to be used for the Histogram \n",
        "        self.bins = bins\n",
        "    \n",
        "    def fcnnInputDim(self):\n",
        "        \"\"\"Calculate The Input Dimension of the FCNN Layer\"\"\"\n",
        "        if self.hist and self.ntn_layer:\n",
        "            return self.bins+self.k\n",
        "        elif self.hist:\n",
        "            return self.bins + 1\n",
        "        elif self.ntn_layer:\n",
        "            return self.k\n",
        "        else : \n",
        "            # Instead of NTN ,we would just be using a simple rbf kernel\n",
        "            # between the Graph Embeddings to Compute Similarities\n",
        "            return 1\n",
        "    \n",
        "    def setupLayers(self):\n",
        "        fcnn_input_dim = self.fcnnInputDim()\n",
        "        \n",
        "        # Layers of SimGNN\n",
        "        # GCN Layers\n",
        "        self.conv1 = GCNConv(self.num_node_features,64)\n",
        "        self.conv2 = GCNConv(64,32)\n",
        "        self.conv3 = GCNConv(32,self.d)\n",
        "        \n",
        "        # Attention Layer and Neural Tensor Network Layer \n",
        "        self.attention_layer = AttentionLayer(self.d)\n",
        "        self.NTN = NTNLayer(self.d, self.k)\n",
        "        \n",
        "        # Fully Connected Layer\n",
        "        self.linear_1 = torch.nn.Linear(fcnn_input_dim,16)\n",
        "        self.linear_2 = torch.nn.Linear(16,8)\n",
        "        self.linear_3 = torch.nn.Linear(8,4)\n",
        "        self.linear_4 = torch.nn.Linear(4,1)\n",
        "    \n",
        "    def GCN(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Implementing the Graph Convolutional Network\n",
        "        :param x : One Hot Encoded Feature Representation of the Nodes\n",
        "        :param edge_index : Tensor Representation of Edges to calculate Adjacency Matrix\n",
        "        :U : (N_B x D) matrix of Node Embeddings\n",
        "        :N_B : Total No. of Nodes in the Batch (irrespective of parent graph) \n",
        "        :D : dimensions of the Node Embeddings (decided by us)\n",
        "        I haven't implemented Dropout/ BatchNorm but can also try to do that\n",
        "        \"\"\"\n",
        "        U = self.conv1(x, edge_index)\n",
        "        U = U.relu()\n",
        "        U = self.conv2(U, edge_index)\n",
        "        U = U.relu()\n",
        "        U = self.conv3(U, edge_index)\n",
        "        return U\n",
        "\n",
        "    def kernel(self, graph_embedding_1, graph_embedding_2):\n",
        "        pass\n",
        "\n",
        "    def rbf_kernel_sim(self, graph_embedding_1, graph_embedding_2):\n",
        "        \"\"\"\n",
        "        :param: graph_embedding_1 : (B,D) dimensional graph embedding\n",
        "        :param: graph_embedding_2 : (B,D) dimensional graph embedding\n",
        "        :return: rbf_sim : RBF Kernel Similarity the two graph embeddings\n",
        "        \"\"\"\n",
        "        distance = graph_embedding_1-graph_embedding_2\n",
        "        distance = torch.sum(distance*distance, dim = 1)\n",
        "        # return (1/distance).view(-1,1)\n",
        "        return torch.exp(-distance).view(-1,1)\n",
        "    \n",
        "    def histogram(self, U1, U2, batch1, batch2):\n",
        "        \"\"\" B = Batch Size\n",
        "        To calculate the Histogram Representation of the Pairwise Interaction Tensor\n",
        "        :param: U1 : (N_B1 x D) matrix which encodes the node embeddings of Graph 1\n",
        "        :param: U2 : (N_B2 x D) matrix which encodes the node embeddings of Graph 2\n",
        "        :param: batch1 : Logs the Parent graph of the Nodes\n",
        "        :param: batch2 : Logs the Parent Graph of the Nodes\n",
        "        :return: norm_hist_scores : (B x self.bins) Normalized histogram for each batch\n",
        "        \"\"\"\n",
        "        # Convert U1 and U2 into Dense Matrices\n",
        "        U1, mask1 = to_dense_batch(U1, batch1) # (B, N_max1, D); (B, N_max1)\n",
        "        U2, mask2 = to_dense_batch(U2, batch2) # (B, N_max2, D); (B, N_max2)\n",
        "        B, N_max1, _ = U1.size()\n",
        "        B, N_max2, _ = U2.size()\n",
        "        \n",
        "        # Max Number of Nodes for Each Graph Pair in the Batch\n",
        "        max_total_nodes = torch.max(mask1.sum(dim=1), mask2.sum(dim=1)).view(-1) # (B,1)\n",
        "         \n",
        "        # Calculating Interaction Scores for the entire batch\n",
        "        interaction_scores = torch.matmul(U1, U2.permute(0,2,1)).detach() # (B, N_max1, N_max2)\n",
        "        interaction_scores = torch.sigmoid(interaction_scores)\n",
        "\n",
        "        # Getting the Histogram for each Pair in the batch\n",
        "        hist_score_list = []\n",
        "        for i in range(B):\n",
        "            interaction_matrix = interaction_scores[:,:max_total_nodes[i], :max_total_nodes[i]]\n",
        "            hist_score = torch.histc(interaction_matrix, bins = self.bins).view(-1) # (self.bins,)\n",
        "            hist_score = hist_score/hist_score.sum() # Normalizing the Histogram\n",
        "            hist_score_list.append(hist_score)\n",
        "\n",
        "        return torch.stack(hist_score_list).view(B, self.bins) # (B, self.bins)\n",
        "    \n",
        "    def FCNN(self, x):\n",
        "        \"\"\":param: x: Rough Similarity Scores of 'B' Graph Pairs where B is Batch Size\"\"\"\n",
        "        ged_sim = self.linear_1(x)\n",
        "        ged_sim = ged_sim.relu()\n",
        "        \n",
        "        ged_sim = self.linear_2(ged_sim)\n",
        "        ged_sim = ged_sim.relu()\n",
        "        \n",
        "        ged_sim = self.linear_3(ged_sim)\n",
        "        ged_sim = ged_sim.relu()\n",
        "        ged_sim = self.linear_4(ged_sim)\n",
        "        \n",
        "        return ged_sim.sigmoid()\n",
        "    \n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "         Forward pass with graphs.\n",
        "         :param data: A Batch Containing a Pair of Graphs.\n",
        "         :return score: Similarity score.\n",
        "         \"\"\"\n",
        "        edge_index_1 = data.edge_index_1\n",
        "        edge_index_2 = data.edge_index_2\n",
        "        x1, x2 = data.x1, data.x2\n",
        "        x1_batch , x2_batch = data.x1_batch, data.x2_batch\n",
        "\n",
        "        # Passed through GCN\n",
        "        node_embeddings_1 = self.GCN(x1, edge_index_1)\n",
        "        node_embeddings_2 = self.GCN(x2, edge_index_2)\n",
        "\n",
        "        # Passed through Attention Layer to get Graph Embedding\n",
        "        graph_embedding_1 = self.attention_layer(node_embeddings_1, x1_batch)\n",
        "        graph_embedding_2 = self.attention_layer(node_embeddings_2, x2_batch)\n",
        "        \n",
        "        # Passed through Neural Tensor Network if allowed otherwise just take a simple Inner Product\n",
        "        if self.ntn_layer:\n",
        "            scores = self.NTN(graph_embedding_1, graph_embedding_2)\n",
        "        else:\n",
        "            scores = self.rbf_kernel_sim(graph_embedding_1, graph_embedding_2)\n",
        "            \n",
        "        # Computed Histogram from the Node Embeddings (Strategy 2)\n",
        "        if self.hist:\n",
        "            hist = self.histogram(node_embeddings_1, node_embeddings_2, x1_batch, x2_batch)\n",
        "            scores = torch.cat((scores, hist), dim=1) # (B, K+self.bins)\n",
        "\n",
        "        # Pass through the Fully Connected Neural Network Layer to get Graph Edit Distance Similarity\n",
        "        sim_pred = self.FCNN(scores)\n",
        "        \n",
        "        return sim_pred.view(-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOSZQfalF-Go"
      },
      "source": [
        "# **4) Loading Data & Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD4z2XVPGCTI",
        "outputId": "5c97e94e-3e79-4bd7-e9af-e28fd2dace7a"
      },
      "source": [
        "name = \"LINUX\"\n",
        "from torch_geometric.datasets import GEDDataset\n",
        "train_graphs = GEDDataset(root=\"./data_/train\", train = True, name=name)\n",
        "test_graphs = GEDDataset(root=\"./data_/test\", train = False, name=name)\n",
        "\n",
        "print(f\"Number of Graphs in Train Set : {len(train_graphs)}\")\n",
        "print(f\"Number of Graphs in Test Set : {len(test_graphs)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://drive.google.com/uc?export=download&id=1nw0RRVgyLpit4V4XFQyDy0pI6wUEXSOI\n",
            "Extracting data_/train/raw/uc\n",
            "Downloading https://drive.google.com/uc?export=download&id=14FDm3NSnrBvB7eNpLeGy5Bz6FjuCSF5v\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://drive.google.com/uc?export=download&id=1nw0RRVgyLpit4V4XFQyDy0pI6wUEXSOI\n",
            "Extracting data_/test/raw/uc\n",
            "Downloading https://drive.google.com/uc?export=download&id=14FDm3NSnrBvB7eNpLeGy5Bz6FjuCSF5v\n",
            "Processing...\n",
            "Done!\n",
            "Number of Graphs in Train Set : 800\n",
            "Number of Graphs in Test Set : 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5qV6Z7KI1aA",
        "outputId": "3e6c52c3-1252-408a-e330-f0ea2db6759c"
      },
      "source": [
        "i = 1\n",
        "print(train_graphs[i])\n",
        "print(test_graphs[i])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 8], i=[1])\n",
            "Data(edge_index=[2, 14], i=[1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfBXgAbqHK2k"
      },
      "source": [
        "## **a) Creating a Feature Matrix for the Graphs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06jh6UrsGHy0",
        "outputId": "b0bb1c6e-1920-4a80-cbd3-ee5cdba90e46"
      },
      "source": [
        "from torch_geometric.transforms import OneHotDegree\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "# If the dataset does not have a feature matrix, we create one!\n",
        "# Only the AIDS700nef Dataset has an inbuilt feature matrix\n",
        "# We create a One Hot Encoded Degree Feature Matrix\n",
        "if train_graphs[0].x is None:\n",
        "            max_degree = 0\n",
        "            for graph in train_graphs + test_graphs:\n",
        "                # If this graph has edges then do\n",
        "                if graph.edge_index.size(1) > 0:\n",
        "                    max_degree = max(max_degree, int(degree(graph.edge_index[0]).max().item()))\n",
        "            \n",
        "            # Create the feature matrix for the Dataset\n",
        "            one_hot_degree = OneHotDegree(max_degree, cat=False)\n",
        "            train_graphs.transform = one_hot_degree\n",
        "            test_graphs.transform = one_hot_degree\n",
        "\n",
        "num_node_features = train_graphs.num_features\n",
        "num_edge_features = train_graphs.num_edge_features\n",
        "print(num_node_features, num_edge_features)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvcxuzuiHECX"
      },
      "source": [
        "## **b) Making Pairs of Graphs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XKhDc76GqdA"
      },
      "source": [
        "class makeGraphPairs(torch_geometric.data.Data):\n",
        "    \"\"\" \n",
        "    :param: edge_index_1 : Edge Index of the First Graph\n",
        "    :param: edge_index_2 : Edge Index of the Second Graph in the pair\n",
        "    :param: x_1 : Feature Matrix of the First Graph in the Pair\n",
        "    :param: x_2 : Feature Matrix of the Second Graph in the Pair\n",
        "       \n",
        "    :returns: torch_geometric.data.Data object which comprises two graphs\n",
        "    \"\"\"\n",
        "    def __init__(self, edge_index_1, x1, edge_index_2, x2, ged, norm_ged, graph_sim):\n",
        "        super(makeGraphPairs, self).__init__()\n",
        "        self.edge_index_1 = edge_index_1\n",
        "        self.x1 = x1\n",
        "        self.edge_index_2 = edge_index_2\n",
        "        self.x2 = x2\n",
        "        self.ged = ged\n",
        "        self.norm_ged = norm_ged\n",
        "        self.graph_sim = graph_sim\n",
        "\n",
        "    def __inc__(self, key, value):\n",
        "        if key == \"edge_index_1\":\n",
        "            return self.x1.size(0)\n",
        "        elif key == \"edge_index_2\":\n",
        "            return self.x2.size(0)\n",
        "        else:\n",
        "            return super().__inc__(key, value)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBTED2zKIC-7"
      },
      "source": [
        "### **i. For Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZW5jrubHaff"
      },
      "source": [
        "\"\"\"TRAINING SET PAIR\"\"\"\n",
        "# Data List to pass into the Data Loader to get Batches\n",
        "train_graph_pair_list = []\n",
        "\n",
        "# Making the Pairs of Graphs\n",
        "for graph1_num, graph1 in enumerate(train_graphs):\n",
        "    for graph2 in train_graphs:\n",
        "        # Initializing Data\n",
        "        edge_index_1 = graph1.edge_index\n",
        "        x1 = graph1.x\n",
        "        edge_index_2 = graph2.edge_index\n",
        "        x2 = graph2.x\n",
        "        ged = train_graphs.ged[graph1.i, graph2.i]\n",
        "        norm_ged = train_graphs.norm_ged[graph1.i, graph2.i]\n",
        "        graph_sim = torch.exp(-norm_ged)\n",
        "        \n",
        "        # Making Graph Pair\n",
        "        graph_pair = makeGraphPairs(edge_index_1=edge_index_1, x1=x1, \n",
        "                                    edge_index_2=edge_index_2, x2=x2,\n",
        "                                    ged=ged ,norm_ged=norm_ged, graph_sim = graph_sim)\n",
        "        \n",
        "        # Saving all the Graph Pairs to the List for Batching and Data Loading\n",
        "        train_graph_pair_list.append(graph_pair)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOJWjpv6IHZ2"
      },
      "source": [
        "### **ii. For Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ak5ROuiHlgd"
      },
      "source": [
        "\"\"\"TEST SET PAIR\"\"\"\n",
        "# Data List to pass into the Data Loader to get batches\n",
        "test_graph_pair_list = []\n",
        "\n",
        "# Making the Pairs of Graphs\n",
        "for graph1 in test_graphs:\n",
        "    for graph2 in train_graphs:\n",
        "        # Initializing Data\n",
        "        edge_index_1 = graph1.edge_index\n",
        "        x1 = graph1.x\n",
        "        edge_index_2 = graph2.edge_index\n",
        "        x2 = graph2.x\n",
        "        ged = train_graphs.ged[graph1.i, graph2.i]\n",
        "        norm_ged = train_graphs.norm_ged[graph1.i, graph2.i]\n",
        "        graph_sim = torch.exp(-norm_ged)\n",
        "        \n",
        "        # Making Graph Pair\n",
        "        graph_pair = makeGraphPairs(edge_index_1=edge_index_1, x1=x1, \n",
        "                                    edge_index_2=edge_index_2, x2=x2,\n",
        "                                    ged=ged ,norm_ged=norm_ged, graph_sim = graph_sim)\n",
        "        \n",
        "        # Saving all the Graph Pairs to the List for Batching and Data Loading\n",
        "        test_graph_pair_list.append(graph_pair)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFslQnWbIOhb",
        "outputId": "1718d45c-41d7-4624-cfdb-f8d904663dec"
      },
      "source": [
        "print(\"Number of Training Graph Pairs = {}\".format(len(train_graph_pair_list)))\n",
        "print(\"Number of Training Test Pairs = {}\".format(len(test_graph_pair_list)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Graph Pairs = 640000\n",
            "Number of Training Test Pairs = 160000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEQFPgVHJdeh",
        "outputId": "c7d90b54-6f17-45d7-d6f5-8be5d7fec403"
      },
      "source": [
        "test_graph_pair_list[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "makeGraphPairs(edge_index_1=[2, 6], edge_index_2=[2, 8], ged=[1], graph_sim=[1], norm_ged=[1], x1=[4, 8], x2=[5, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waUgvHJIIeLe"
      },
      "source": [
        "# **5) Training Begins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeJRi1rzjK8b",
        "outputId": "d4f01518-bd82-4c20-fafe-1d84548515a9"
      },
      "source": [
        "import random\n",
        "\n",
        "# Creating Validation Data\n",
        "val_graph_pair_list = random.sample(train_graph_pair_list, len(test_graph_pair_list))\n",
        "# Removing the Validation examples from Train Set\n",
        "train_graph_pair_list = list(set(train_graph_pair_list)-set(val_graph_pair_list))\n",
        "\n",
        "print(\"Number of Training Graph Pairs = {}\".format(len(train_graph_pair_list)))\n",
        "print(\"Number of Validation Graph Pairs = {}\".format(len(val_graph_pair_list)))\n",
        "print(\"Number of Test Graph Pairs = {}\".format(len(test_graph_pair_list)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Graph Pairs = 480000\n",
            "Number of Validation Graph Pairs = 160000\n",
            "Number of Test Graph Pairs = 160000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUUj2aTav0zt"
      },
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "test_loader = DataLoader(test_graph_pair_list, batch_size = 1024, follow_batch = [\"x1\", \"x2\"], shuffle = True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnCYdo1VJ2uy"
      },
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_graph_pair_list, batch_size = batch_size, follow_batch = [\"x1\", \"x2\"], shuffle = True)\n",
        "val_loader = DataLoader(val_graph_pair_list, batch_size = batch_size, follow_batch= [\"x1\", \"x2\"], shuffle = True)\n",
        "test_loader = DataLoader(test_graph_pair_list, batch_size = 1024, follow_batch = [\"x1\", \"x2\"], shuffle = True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QUJqmRwKPiH",
        "outputId": "8c8af680-31f0-44ac-b192-ac38f28163ce"
      },
      "source": [
        "num_batches = len(train_loader)\n",
        "print(len(train_loader))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsxsA1PtR85B"
      },
      "source": [
        "def evaluate(dataloader, model, loss_criterion):\n",
        "    total_loss = 0\n",
        "    num_ex = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            data = data.to(device)\n",
        "            sim_pred = model(data)\n",
        "            batch_loss = loss_criterion(sim_pred, data.graph_sim)\n",
        "            total_loss = total_loss + batch_loss*len(data.ged)\n",
        "            num_ex += len(data.ged)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "    return total_loss.item()/num_ex"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6Tza6rxIn3v",
        "outputId": "512e4138-8b09-44c8-92c3-c00c7839246b"
      },
      "source": [
        "model = SimGNN(num_node_features, ntn_layer=True, hist=True).to(device)\n",
        "loss_criterion = torch.nn.MSELoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 5e-6)\n",
        "\n",
        "epochs = 7\n",
        "min_val_loss = 1000\n",
        "min_train_loss = 0\n",
        "num_batches = len(train_loader) \n",
        "\n",
        "import time\n",
        "import copy\n",
        "from itertools import cycle\n",
        "train_loss_arr = []\n",
        "val_loss_arr = []\n",
        "tik = time.time()\n",
        "\n",
        "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    for i, (train_batch, val_batch) in enumerate(zip(train_loader, cycle(val_loader))):\n",
        "        # Training the Model\n",
        "        opt.zero_grad()\n",
        "        train_batch = train_batch.to(device)\n",
        "        y_pred = model(train_batch)\n",
        "        loss = loss_criterion(y_pred.view(-1), train_batch.graph_sim)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        train_loss_arr.append(loss.item())\n",
        "\n",
        "        #Cross Validation Begins\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_batch = val_batch.to(device)\n",
        "            y_val_pred = model(val_batch)\n",
        "            val_loss = loss_criterion(y_val_pred.view(-1), val_batch.graph_sim)\n",
        "            val_loss_arr.append(val_loss.item())\n",
        "        model.train()\n",
        "        \n",
        "        # Model Checkpointing\n",
        "        if (min_val_loss>loss.item()) and (min_val_loss>val_loss.item()):\n",
        "            min_val_loss = val_loss.item()\n",
        "            min_train_loss = loss.item() # Just saving this for reference\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        # Printing Loss Values\n",
        "        if i%200 == 0:\n",
        "            print(f\"Epoch{epoch+1}/{epochs} | Batch: {i}/{num_batches} | Train Loss: {loss} | Validation Loss: {val_loss}\")\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache() \n",
        "    \n",
        "    # Printing Epoch Summary\n",
        "    print(f\"Epoch: {epoch+1}/{epochs} | Train MSE: {loss} | Validation MSE: {val_loss}\")\n",
        "\n",
        "tok = time.time()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpochs:   0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch1/7 | Batch: 0/3750 | Train Loss: 0.03278675675392151 | Validation Loss: 0.034326568245887756\n",
            "Epoch1/7 | Batch: 300/3750 | Train Loss: 0.009013151749968529 | Validation Loss: 0.009240610525012016\n",
            "Epoch1/7 | Batch: 600/3750 | Train Loss: 0.006669340655207634 | Validation Loss: 0.006871847435832024\n",
            "Epoch1/7 | Batch: 900/3750 | Train Loss: 0.006431351415812969 | Validation Loss: 0.005360754672437906\n",
            "Epoch1/7 | Batch: 1200/3750 | Train Loss: 0.003210979513823986 | Validation Loss: 0.00591296935454011\n",
            "Epoch1/7 | Batch: 1500/3750 | Train Loss: 0.004010442644357681 | Validation Loss: 0.0032377871684730053\n",
            "Epoch1/7 | Batch: 1800/3750 | Train Loss: 0.002751885913312435 | Validation Loss: 0.0020669461227953434\n",
            "Epoch1/7 | Batch: 2100/3750 | Train Loss: 0.0034725614823400974 | Validation Loss: 0.001939358888193965\n",
            "Epoch1/7 | Batch: 2400/3750 | Train Loss: 0.0023479594383388758 | Validation Loss: 0.0036976838018745184\n",
            "Epoch1/7 | Batch: 2700/3750 | Train Loss: 0.004400455392897129 | Validation Loss: 0.0019663451239466667\n",
            "Epoch1/7 | Batch: 3000/3750 | Train Loss: 0.0018005544552579522 | Validation Loss: 0.003026832826435566\n",
            "Epoch1/7 | Batch: 3300/3750 | Train Loss: 0.0020968313328921795 | Validation Loss: 0.0017716174479573965\n",
            "Epoch1/7 | Batch: 3600/3750 | Train Loss: 0.0019257894018664956 | Validation Loss: 0.0013633091002702713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpochs:  14%|█▍        | 1/7 [05:28<32:53, 328.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/7 | Train MSE: 0.0017225089250132442 | Validation MSE: 0.001607581041753292\n",
            "Epoch2/7 | Batch: 0/3750 | Train Loss: 0.0022211680188775063 | Validation Loss: 0.0014292303239926696\n",
            "Epoch2/7 | Batch: 300/3750 | Train Loss: 0.003005571197718382 | Validation Loss: 0.0018651967402547598\n",
            "Epoch2/7 | Batch: 600/3750 | Train Loss: 0.002398404059931636 | Validation Loss: 0.0020655717235058546\n",
            "Epoch2/7 | Batch: 900/3750 | Train Loss: 0.002228851430118084 | Validation Loss: 0.002371616428717971\n",
            "Epoch2/7 | Batch: 1200/3750 | Train Loss: 0.0017193108797073364 | Validation Loss: 0.001791043090634048\n",
            "Epoch2/7 | Batch: 1500/3750 | Train Loss: 0.0015690376749262214 | Validation Loss: 0.0016447678208351135\n",
            "Epoch2/7 | Batch: 1800/3750 | Train Loss: 0.0018239018972963095 | Validation Loss: 0.0016651346813887358\n",
            "Epoch2/7 | Batch: 2100/3750 | Train Loss: 0.0015804925933480263 | Validation Loss: 0.0016971636796370149\n",
            "Epoch2/7 | Batch: 2400/3750 | Train Loss: 0.0019070494454354048 | Validation Loss: 0.0018110580276697874\n",
            "Epoch2/7 | Batch: 2700/3750 | Train Loss: 0.0021078933496028185 | Validation Loss: 0.0016623310511931777\n",
            "Epoch2/7 | Batch: 3000/3750 | Train Loss: 0.0020416644401848316 | Validation Loss: 0.0012981337495148182\n",
            "Epoch2/7 | Batch: 3300/3750 | Train Loss: 0.001565528684295714 | Validation Loss: 0.0011516358936205506\n",
            "Epoch2/7 | Batch: 3600/3750 | Train Loss: 0.0017483775736764073 | Validation Loss: 0.001313567627221346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpochs:  29%|██▊       | 2/7 [10:52<27:17, 327.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2/7 | Train MSE: 0.0010500111384317279 | Validation MSE: 0.0014127700123935938\n",
            "Epoch3/7 | Batch: 0/3750 | Train Loss: 0.0014127623289823532 | Validation Loss: 0.0012731964234262705\n",
            "Epoch3/7 | Batch: 300/3750 | Train Loss: 0.002131946850568056 | Validation Loss: 0.0011719779577106237\n",
            "Epoch3/7 | Batch: 600/3750 | Train Loss: 0.0011764980154111981 | Validation Loss: 0.0010900641791522503\n",
            "Epoch3/7 | Batch: 900/3750 | Train Loss: 0.0009140627807937562 | Validation Loss: 0.0008355013560503721\n",
            "Epoch3/7 | Batch: 1200/3750 | Train Loss: 0.0013230452314019203 | Validation Loss: 0.0016434859717264771\n",
            "Epoch3/7 | Batch: 1500/3750 | Train Loss: 0.0008646808564662933 | Validation Loss: 0.0010916328756138682\n",
            "Epoch3/7 | Batch: 1800/3750 | Train Loss: 0.0015367348678410053 | Validation Loss: 0.0008421125821769238\n",
            "Epoch3/7 | Batch: 2100/3750 | Train Loss: 0.001285931677557528 | Validation Loss: 0.0016194852069020271\n",
            "Epoch3/7 | Batch: 2400/3750 | Train Loss: 0.0007554538315162063 | Validation Loss: 0.0014073906932026148\n",
            "Epoch3/7 | Batch: 2700/3750 | Train Loss: 0.0011698189191520214 | Validation Loss: 0.0006958264857530594\n",
            "Epoch3/7 | Batch: 3000/3750 | Train Loss: 0.0006446529878303409 | Validation Loss: 0.0011014530900865793\n",
            "Epoch3/7 | Batch: 3300/3750 | Train Loss: 0.0012743908446282148 | Validation Loss: 0.0016815682174637914\n",
            "Epoch3/7 | Batch: 3600/3750 | Train Loss: 0.0010253828950226307 | Validation Loss: 0.0009423231240361929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpochs:  43%|████▎     | 3/7 [16:18<21:48, 327.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3/7 | Train MSE: 0.0008731387206353247 | Validation MSE: 0.0011668183142319322\n",
            "Epoch4/7 | Batch: 0/3750 | Train Loss: 0.0009672763990238309 | Validation Loss: 0.0010472660651430488\n",
            "Epoch4/7 | Batch: 300/3750 | Train Loss: 0.0007060910575091839 | Validation Loss: 0.0007883164216764271\n",
            "Epoch4/7 | Batch: 600/3750 | Train Loss: 0.0007604941492900252 | Validation Loss: 0.0008817848283797503\n",
            "Epoch4/7 | Batch: 900/3750 | Train Loss: 0.001170661300420761 | Validation Loss: 0.0006995787844061852\n",
            "Epoch4/7 | Batch: 1200/3750 | Train Loss: 0.0015811463817954063 | Validation Loss: 0.0007370223174802959\n",
            "Epoch4/7 | Batch: 1500/3750 | Train Loss: 0.001324727083556354 | Validation Loss: 0.000999106909148395\n",
            "Epoch4/7 | Batch: 1800/3750 | Train Loss: 0.0009308444568887353 | Validation Loss: 0.0005217466969043016\n",
            "Epoch4/7 | Batch: 2100/3750 | Train Loss: 0.0006185213569551706 | Validation Loss: 0.001047021709382534\n",
            "Epoch4/7 | Batch: 2400/3750 | Train Loss: 0.0007798456936143339 | Validation Loss: 0.0006258813082240522\n",
            "Epoch4/7 | Batch: 2700/3750 | Train Loss: 0.0007427108939737082 | Validation Loss: 0.0012553648557513952\n",
            "Epoch4/7 | Batch: 3000/3750 | Train Loss: 0.0006630138377659023 | Validation Loss: 0.000715946895070374\n",
            "Epoch4/7 | Batch: 3300/3750 | Train Loss: 0.0006527950172312558 | Validation Loss: 0.0006331549375317991\n",
            "Epoch4/7 | Batch: 3600/3750 | Train Loss: 0.0006868379423394799 | Validation Loss: 0.0006062257452867925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpochs:  57%|█████▋    | 4/7 [21:44<16:19, 326.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4/7 | Train MSE: 0.0010094877798110247 | Validation MSE: 0.0009793346980586648\n",
            "Epoch5/7 | Batch: 0/3750 | Train Loss: 0.0009391977800987661 | Validation Loss: 0.0011233394034206867\n",
            "Epoch5/7 | Batch: 300/3750 | Train Loss: 0.0007810573442839086 | Validation Loss: 0.0006540298345498741\n",
            "Epoch5/7 | Batch: 600/3750 | Train Loss: 0.0006868450436741114 | Validation Loss: 0.001045807497575879\n",
            "Epoch5/7 | Batch: 900/3750 | Train Loss: 0.0013826085487380624 | Validation Loss: 0.0007707441691309214\n",
            "Epoch5/7 | Batch: 1200/3750 | Train Loss: 0.0012169622350484133 | Validation Loss: 0.0008495693909935653\n",
            "Epoch5/7 | Batch: 1500/3750 | Train Loss: 0.0007409242098219693 | Validation Loss: 0.0006542467162944376\n",
            "Epoch5/7 | Batch: 1800/3750 | Train Loss: 0.0007984405383467674 | Validation Loss: 0.000608094094786793\n",
            "Epoch5/7 | Batch: 2100/3750 | Train Loss: 0.000644552696030587 | Validation Loss: 0.0012788670137524605\n",
            "Epoch5/7 | Batch: 2400/3750 | Train Loss: 0.0007334903348237276 | Validation Loss: 0.0007034825976006687\n",
            "Epoch5/7 | Batch: 2700/3750 | Train Loss: 0.0006930651143193245 | Validation Loss: 0.0011132981162518263\n",
            "Epoch5/7 | Batch: 3000/3750 | Train Loss: 0.0004784436314366758 | Validation Loss: 0.0009788984898477793\n",
            "Epoch5/7 | Batch: 3300/3750 | Train Loss: 0.0008463615085929632 | Validation Loss: 0.0007825340144336224\n",
            "Epoch5/7 | Batch: 3600/3750 | Train Loss: 0.001036261091940105 | Validation Loss: 0.0007572315516881645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpochs:  71%|███████▏  | 5/7 [27:10<10:52, 326.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/7 | Train MSE: 0.0005352775915525854 | Validation MSE: 0.0006294577615335584\n",
            "Epoch6/7 | Batch: 0/3750 | Train Loss: 0.000763817923143506 | Validation Loss: 0.0005340100033208728\n",
            "Epoch6/7 | Batch: 300/3750 | Train Loss: 0.0006387820467352867 | Validation Loss: 0.0009432226070202887\n",
            "Epoch6/7 | Batch: 600/3750 | Train Loss: 0.0006161160999909043 | Validation Loss: 0.0006298115476965904\n",
            "Epoch6/7 | Batch: 900/3750 | Train Loss: 0.0015320872189477086 | Validation Loss: 0.0006010217475704849\n",
            "Epoch6/7 | Batch: 1200/3750 | Train Loss: 0.0008970369817689061 | Validation Loss: 0.0011703020427376032\n",
            "Epoch6/7 | Batch: 1500/3750 | Train Loss: 0.000642991391941905 | Validation Loss: 0.0007123446557670832\n",
            "Epoch6/7 | Batch: 1800/3750 | Train Loss: 0.0006672046147286892 | Validation Loss: 0.0005879299715161324\n",
            "Epoch6/7 | Batch: 2100/3750 | Train Loss: 0.000884806620888412 | Validation Loss: 0.000645429128780961\n",
            "Epoch6/7 | Batch: 2400/3750 | Train Loss: 0.0011094409273937345 | Validation Loss: 0.0007214670768007636\n",
            "Epoch6/7 | Batch: 2700/3750 | Train Loss: 0.0009524619672447443 | Validation Loss: 0.0004597156075760722\n",
            "Epoch6/7 | Batch: 3000/3750 | Train Loss: 0.000565136200748384 | Validation Loss: 0.0006772970082238317\n",
            "Epoch6/7 | Batch: 3300/3750 | Train Loss: 0.0005963232251815498 | Validation Loss: 0.0004415028088260442\n",
            "Epoch6/7 | Batch: 3600/3750 | Train Loss: 0.000551757519133389 | Validation Loss: 0.0004895230522379279\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpochs:  86%|████████▌ | 6/7 [32:33<05:25, 325.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6/7 | Train MSE: 0.0005445805727504194 | Validation MSE: 0.0006356885423883796\n",
            "Epoch7/7 | Batch: 0/3750 | Train Loss: 0.0005103161092847586 | Validation Loss: 0.0007732708472758532\n",
            "Epoch7/7 | Batch: 300/3750 | Train Loss: 0.0007088122656568885 | Validation Loss: 0.0006299025262705982\n",
            "Epoch7/7 | Batch: 600/3750 | Train Loss: 0.0005204372573643923 | Validation Loss: 0.00047633005306124687\n",
            "Epoch7/7 | Batch: 900/3750 | Train Loss: 0.0005289911641739309 | Validation Loss: 0.0003928783698938787\n",
            "Epoch7/7 | Batch: 1200/3750 | Train Loss: 0.00039527344051748514 | Validation Loss: 0.0006296292413026094\n",
            "Epoch7/7 | Batch: 1500/3750 | Train Loss: 0.0005651228129863739 | Validation Loss: 0.0008283949573524296\n",
            "Epoch7/7 | Batch: 1800/3750 | Train Loss: 0.0006444610189646482 | Validation Loss: 0.0005474882200360298\n",
            "Epoch7/7 | Batch: 2100/3750 | Train Loss: 0.00027153349947184324 | Validation Loss: 0.0006314297206699848\n",
            "Epoch7/7 | Batch: 2400/3750 | Train Loss: 0.000289485149551183 | Validation Loss: 0.0005718686734326184\n",
            "Epoch7/7 | Batch: 2700/3750 | Train Loss: 0.000535217288415879 | Validation Loss: 0.0007661269046366215\n",
            "Epoch7/7 | Batch: 3000/3750 | Train Loss: 0.0005701978807337582 | Validation Loss: 0.000667211483232677\n",
            "Epoch7/7 | Batch: 3300/3750 | Train Loss: 0.0007026088424026966 | Validation Loss: 0.0009278399520553648\n",
            "Epoch7/7 | Batch: 3600/3750 | Train Loss: 0.0006177423056215048 | Validation Loss: 0.0006581536727026105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 7/7 [38:00<00:00, 325.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7/7 | Train MSE: 0.0005666491342708468 | Validation MSE: 0.0003506544162519276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "K7CBMMWbc1fu",
        "outputId": "8e7d09d6-f2e2-46c1-dfee-1fd5cc6cd02d"
      },
      "source": [
        "num = 0\n",
        "\n",
        "print(\"Minimum Validation Loss: \",min_val_loss, \"\\nMinimum Train Loss: \", min_train_loss)\n",
        "print(\"========================================================\")\n",
        "print(f\"Time taken for Training = {round(tok-tik, 3)}s\")\n",
        "plt.plot(train_loss_arr[num:], label = \"Train Loss\", alpha = 1)\n",
        "plt.plot(val_loss_arr[num:], label = \"Val Loss\", alpha = 0.5)\n",
        "plt.legend()\n",
        "plt.title(\"Loss Curves\")\n",
        "plt.xlabel(\"No. of Iterations\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum Validation Loss:  0.0002520935086067766 \n",
            "Minimum Train Loss:  0.0003301297838333994\n",
            "========================================================\n",
            "Time taken for Training = 2280.134s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lFItoHui0Jz",
        "outputId": "1a7e553c-a4ad-4f41-9e38-c539bb78b341"
      },
      "source": [
        "# Testing the Model\n",
        "model.load_state_dict(best_model)\n",
        "tik = time.time() \n",
        "print(evaluate(test_loader, model, loss_criterion))\n",
        "tok = time.time()\n",
        "print(\"Test Time =\",tok-tik, \"s\")\n",
        "print(\"Avg. Time Per Pair =\",(tok-tik)/len(test_graph_pair_list),\"s\")\n",
        "\n",
        "#torch.save(model,f'simgnn-{name}.pth')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0005287031173706055\n",
            "Test Time = 49.18877410888672 s\n",
            "Avg. Time Per Pair = 0.00030742983818054197 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}