{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn2featuregraph.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/FZiyHIlu9vGqEzVK41Un",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kunind27/Graph-Similarity-Computation/blob/main/nn2featuregraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_EQ5PCwxOtJ"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(color_codes = True)\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q68bddahxq8C"
      },
      "source": [
        "# Downloading and installing Pytorch Geometric\n",
        "#!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "#!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "#!pip install torch-geometric\n",
        "#import torch_geometric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig7-Oo5JsQkX",
        "outputId": "75a77ec5-5252-410a-abae-a3c7f00ff9df"
      },
      "source": [
        "from torch import nn\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Neural Network Specifications : \n",
        "class neural_network(torch.nn.Module):\n",
        "    def __init__(self, input_dim = 784, hidden_layer_dim = 25, output_dim = 10):\n",
        "        super(neural_network, self).__init__()\n",
        "        self.nn_model = nn.Sequential(nn.Linear(input_dim, hidden_layer_dim),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.Linear(hidden_layer_dim, output_dim),\n",
        "                                      nn.Softmax())\n",
        "    def forward(self, input):\n",
        "        return self.nn_model(input)\n",
        "\n",
        "model = neural_network(input_dim = 2, hidden_layer_dim = 3, output_dim = 1)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neural_network(\n",
            "  (nn_model): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "    (3): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMmcbta2sgVK",
        "outputId": "8911e42f-8220-4971-f7a5-3279078a079c"
      },
      "source": [
        "# Demonstrating what the functions I have used do\n",
        "print(list(model.children()),\"\\n\")\n",
        "print(list(model.nn_model.children()),\"\\n\")\n",
        "\n",
        "# State Dict\n",
        "print(model.state_dict(),\"\\n\")\n",
        "print(model.state_dict().keys(),\"\\n\")\n",
        "print(model.state_dict().values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sequential(\n",
            "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (3): Softmax(dim=None)\n",
            ")] \n",
            "\n",
            "[Linear(in_features=2, out_features=3, bias=True), ReLU(), Linear(in_features=3, out_features=1, bias=True), Softmax(dim=None)] \n",
            "\n",
            "OrderedDict([('nn_model.0.weight', tensor([[-0.1474,  0.0385],\n",
            "        [ 0.5293, -0.2693],\n",
            "        [ 0.7033,  0.0281]])), ('nn_model.0.bias', tensor([-0.3902, -0.1947,  0.1559])), ('nn_model.2.weight', tensor([[-0.1351,  0.5443,  0.4106]])), ('nn_model.2.bias', tensor([0.1212]))]) \n",
            "\n",
            "odict_keys(['nn_model.0.weight', 'nn_model.0.bias', 'nn_model.2.weight', 'nn_model.2.bias']) \n",
            "\n",
            "odict_values([tensor([[-0.1474,  0.0385],\n",
            "        [ 0.5293, -0.2693],\n",
            "        [ 0.7033,  0.0281]]), tensor([-0.3902, -0.1947,  0.1559]), tensor([[-0.1351,  0.5443,  0.4106]]), tensor([0.1212])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljNZJdipp60S",
        "outputId": "3ca74b66-3732-41ac-a2e6-ec835b17d9b8"
      },
      "source": [
        "list(model.nn_model.state_dict().values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[-0.1474,  0.0385],\n",
              "         [ 0.5293, -0.2693],\n",
              "         [ 0.7033,  0.0281]]),\n",
              " tensor([-0.3902, -0.1947,  0.1559]),\n",
              " tensor([[-0.1351,  0.5443,  0.4106]]),\n",
              " tensor([0.1212])]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4N7N9puVgl"
      },
      "source": [
        "Assume that we have a sample $\\mathbf{X}^{(l)} = \\{\\mathbf{x}^{(l)}_1, \\ldots, \\mathbf{x}^{(l)}_N\\}, \\mbox{where $N$ is total num of datapoints} ,\\ \\mathbf{x}^{(l)}_n = (x_{n1}^{(l)}, \\ldots, x_{nm_l}^{(l)}) \\in \\mathbb{R}^{m_l}$ that is the input for layer $l$ with the dimension $m_l$. We operate with importance of the connection between  the neuron $i$ of the layer $l$ and the neuron $j$ of the layer $l+1$, which we define as: \n",
        "\\begin{equation}\n",
        "    s_{ij}^{(l)} = \\frac{|w_{ij}^{(l)}| \\overline{|x_{i}^{(l)}|}}{\\sum_{k=1}^{m_l} |w^{(l)}_{kj}| \\overline{|x_{k}^{(l)}|}+|b_j^{(l+1)}|},\n",
        "\\end{equation}\n",
        "\n",
        "where $\\overline{|x_{i}^{(l)}|} = \\frac{1}{N}\\sum_{n=1}^N |x_{ni}^{(l)}|$. \n",
        "\n",
        "The total importance in the neuron $j$ of the layer $l+1$ we define as $S^{(l)}_j = \\sum_{k=1}^{m_l} |w^{(l)}_{kj}| \\overline{|x_{k}^{(l)}|}+|b_j^{(l+1)}|, \\ 1 \\le j \\le m_{l+1}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8kguTxxwgV5",
        "outputId": "c8e07dbf-cda6-47a6-fb1a-c65ba8227728"
      },
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randn(2,2)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.76405235 0.40015721]\n",
            " [0.97873798 2.2408932 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVuDovJbuWvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ca46c9-cba9-421c-e963-0a2e930b19c4"
      },
      "source": [
        "def compute_importances(model, X, device = torch.device(\"cpu\"), mode = None):\n",
        "    importance_scores = [] # X = m x 784\n",
        "    neuron_importance_scores = []\n",
        "    neuron_list = []\n",
        "    names = list(model.nn_model.state_dict())    # Names of the parameters\n",
        "    layers = list(model.nn_model.children())  # List of layers, e.g. Linear, ReLU, BatchNorm etc\n",
        "\n",
        "    l = 0\n",
        "    \n",
        "    if mode == \"ignore_first_layer\" : \n",
        "        weight_key = f'{2*l}.weight'\n",
        "        bias_key = f'{2*l}.bias'\n",
        "        fc_weights = model.nn_model.state_dict()[weight_key] # n_l+1 x n_l\n",
        "        fc_biases = model.nn_model.state_dict()[bias_key]\n",
        "\n",
        "        # Score = Average strengths of the signal for every connection in the layer\n",
        "        score = (X.abs().mean(dim=0)*fc_weights.abs()).cpu().detach().numpy().T # (m_l x m_l+1)\n",
        "        total_neuron_importance = score.sum(axis=0) + fc_biases.abs().cpu().detach().numpy()\n",
        "        # score = score/total_neuron_importance\n",
        "        \n",
        "        # Storing all the edge and Neuronal Importances\n",
        "        # importance_scores.append(score)\n",
        "        neuron_importance_scores.append(total_neuron_importance)\n",
        "\n",
        "        X = layers[2*l+1](layers[2*l](X)) \n",
        "        l += 1\n",
        "    \n",
        "    # Computing Importance Scores for Each Layer : 2*(l+1) since linear layer is followed by an activation\n",
        "    while (2*(l+1) < len(layers)):\n",
        "        weight_key = f'{2*l}.weight'\n",
        "        bias_key = f'{2*l}.bias'\n",
        "\n",
        "        fc_weights = model.nn_model.state_dict()[weight_key] # n_l+1 x n_l\n",
        "        fc_biases = model.nn_model.state_dict()[bias_key]\n",
        "\n",
        "        # Score = Average strengths of the signal for every connection in the layer\n",
        "        score = (X.abs().mean(dim=0)*fc_weights.abs()).cpu().detach().numpy().T # (m_l x m_l+1)\n",
        "        total_neuron_importance = score.sum(axis=0) + fc_biases.abs().cpu().detach().numpy()\n",
        "        # Normalize by the total neuron importance. So, the sum of importance sores for incoming connections == 1\n",
        "        # score = score/total_neuron_importance\n",
        "        \n",
        "        # Storing all the edge and Neuronal Importances\n",
        "        importance_scores.append(score)\n",
        "        neuron_importance_scores.append(total_neuron_importance)\n",
        "\n",
        "        # Storing number of Neurons in Current Layer\n",
        "        neurons_curr_layer = fc_weights.shape[1]\n",
        "        neuron_list.append(neurons_curr_layer)\n",
        "\n",
        "        # Propagate signal forward: Linear, Activation\n",
        "        X = layers[2*l+1](layers[2*l](X)) \n",
        "        l += 1   \n",
        "    \n",
        "    # For the Last Layer in the Network\n",
        "    weight_key = f'{2*l}.weight'\n",
        "    bias_key = f'{2*l}.bias'\n",
        "    fc_weights = model.nn_model.state_dict()[weight_key] # n_l+1 x n_l\n",
        "    fc_biases = model.nn_model.state_dict()[bias_key]\n",
        "\n",
        "    score = (X.abs().mean(dim=0)*fc_weights.abs()).cpu().detach().numpy().T\n",
        "    total_neuron_importance = score.sum(axis=0) + fc_biases.abs().cpu().detach().numpy()\n",
        "    # score = score/total_neuron_importance # - Importance Score Normalization\n",
        "    \n",
        "    importance_scores.append(score)   \n",
        "    neuron_importance_scores.append(total_neuron_importance)\n",
        "\n",
        "    neuron_list.extend([fc_weights.shape[1], fc_weights.shape[0]])\n",
        "\n",
        "    # Propagate signal forward: Linear, Activation\n",
        "    X = layers[2*l+1](layers[2*l](X))\n",
        "    \n",
        "    return importance_scores#, neuron_importance_scores, neuron_list#, X\n",
        "\n",
        "compute_importances(model, torch.tensor(X, dtype = torch.float32), device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.6965238 , 0.8336127 , 0.3208684 ],\n",
              "        [0.6123091 , 0.8518192 , 0.85613644]], dtype=float32),\n",
              " array([[0.01108351],\n",
              "        [0.21441086],\n",
              "        [0.03187966]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTww28Q90NZP",
        "outputId": "00a30c3f-047e-4411-9427-fa87e0d9ca29"
      },
      "source": [
        "list(model.nn_model.state_dict().values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[-0.1474,  0.0385],\n",
              "         [ 0.5293, -0.2693],\n",
              "         [ 0.7033,  0.0281]]),\n",
              " tensor([-0.3902, -0.1947,  0.1559]),\n",
              " tensor([[-0.1351,  0.5443,  0.4106]]),\n",
              " tensor([0.1212])]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOnSXygywoQo"
      },
      "source": [
        "# To get List of Neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7YXg8ljLbzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4773ce-950f-480a-eeec-6cdde77b1bca"
      },
      "source": [
        "# Returns the Edges of the NN-Graph in the Edge List form\n",
        "def list_of_neurons(model, mode = None):    \n",
        "    l=0\n",
        "    neuron_list = []\n",
        "    #edge_list = []\n",
        "    layers = list(model.nn_model.children())\n",
        "\n",
        "    if mode == \"ignore_first_layer\":\n",
        "        l+=1\n",
        "\n",
        "    # Computing Importance Scores for Each Layer : 2*(l+1) since linear layer is followed by an activation\n",
        "    while (2*(l+1) < len(layers)):\n",
        "        weight_key = f'{2*l}.weight'\n",
        "        bias_key = f'{2*l}.weight'\n",
        "\n",
        "        fc_weights = model.nn_model.state_dict()[weight_key] # n_l+1 x n_l\n",
        "        fc_biases = model.nn_model.state_dict()[bias_key]\n",
        "        n_curr_layer = fc_weights.shape[1]\n",
        "        neuron_list.append(n_curr_layer) \n",
        "        l+=1\n",
        "\n",
        "    # For last layer\n",
        "    weight_key = f'{2*l}.weight'\n",
        "    fc_weights = model.nn_model.state_dict()[weight_key]\n",
        "    neuron_list.extend([fc_weights.shape[1], fc_weights.shape[0]])\n",
        "    print(neuron_list)\n",
        "    #print(edge_list)\n",
        "\n",
        "list_of_neurons(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSZp6ocCYcRn"
      },
      "source": [
        "# To make Edge List Given Neuron List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDyuhKVO6q4p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsifIK3xYscP",
        "outputId": "205a066b-6fd7-46b9-b421-a210fc253f9f"
      },
      "source": [
        "def unordered_edge_list(nl):\n",
        "    # nl -> neuron list\n",
        "    total_neurons = 0\n",
        "    edge_list = []\n",
        "\n",
        "    # Iterating over Neuron Layers\n",
        "    for layer in range(start, len(nl)-1):\n",
        "        total_neurons += nl[layer]\n",
        "\n",
        "        # Adding Unordered Edge_List for the Layer\n",
        "        for i in range(total_neurons-nl[layer], total_neurons):\n",
        "            for j in range(total_neurons, total_neurons + nl[layer+1]):\n",
        "                edge_list.append([i,j])\n",
        "    \n",
        "    edge_list.extend([[j,i] for i,j in edge_list])\n",
        "    \n",
        "    return (np.array((edge_list)))\n",
        "\n",
        "\n",
        "unordered_edge_list([2,3,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 2],\n",
              "       [0, 3],\n",
              "       [0, 4],\n",
              "       [1, 2],\n",
              "       [1, 3],\n",
              "       [1, 4],\n",
              "       [2, 5],\n",
              "       [3, 5],\n",
              "       [4, 5],\n",
              "       [2, 0],\n",
              "       [3, 0],\n",
              "       [4, 0],\n",
              "       [2, 1],\n",
              "       [3, 1],\n",
              "       [4, 1],\n",
              "       [5, 2],\n",
              "       [5, 3],\n",
              "       [5, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aundNTrsTZT1",
        "outputId": "869d9803-66ce-4b18-c60a-f3429e2bdf5c"
      },
      "source": [
        "def edge_list(nl):    \n",
        "    # nl -> neuron list\n",
        "    total_neurons = 0\n",
        "    edge_list = []\n",
        "\n",
        "    total_neurons += nl[0]\n",
        "    # Adding Edge_List for First Layer\n",
        "    for i in range(total_neurons-nl[0], total_neurons):\n",
        "        for j in range(total_neurons, total_neurons + nl[1]):\n",
        "            edge_list.append([i,j])\n",
        "\n",
        "    for layer in range(1,len(nl)-1): # Exclude First and Last NN Layers\n",
        "        total_neurons += nl[layer]\n",
        "\n",
        "        for i in range(total_neurons-nl[layer], total_neurons):\n",
        "            for j in range(total_neurons - nl[layer] - nl[layer-1], total_neurons - nl[layer]):\n",
        "                edge_list.append([i,j])\n",
        "            for j in range(total_neurons, total_neurons + nl[layer+1]):\n",
        "                edge_list.append([i,j])\n",
        "\n",
        "    # Adding Edge_List for Last Layer\n",
        "    total_neurons += nl[-1]\n",
        "    for i in range(total_neurons-nl[-1], total_neurons):\n",
        "        for j in range(total_neurons - nl[-1] - nl[-2], total_neurons - nl[-1]):\n",
        "                edge_list.append([i,j])\n",
        "    \n",
        "    return np.array(edge_list) # Since PyTorch Geometric Edge List is (2, num_edges)\n",
        "\n",
        "edge_list([2,3,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 2],\n",
              "       [0, 3],\n",
              "       [0, 4],\n",
              "       [1, 2],\n",
              "       [1, 3],\n",
              "       [1, 4],\n",
              "       [2, 0],\n",
              "       [2, 1],\n",
              "       [2, 5],\n",
              "       [3, 0],\n",
              "       [3, 1],\n",
              "       [3, 5],\n",
              "       [4, 0],\n",
              "       [4, 1],\n",
              "       [4, 5],\n",
              "       [5, 2],\n",
              "       [5, 3],\n",
              "       [5, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O92Cfk9swrMp"
      },
      "source": [
        "# To get List of Weights in the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZZD69yAx-VY",
        "outputId": "7a081f63-1800-44d2-eded-b8239bc10176"
      },
      "source": [
        "# Returns a list of weights for each layer in the format : (m_l x m_l+1)\n",
        "def list_of_weights(model, device = torch.device(\"cpu\"), mode = None):\n",
        "    weight_list = None\n",
        "\n",
        "    if mode == \"ignore_first_layer\":\n",
        "        weight_list = list(model.nn_model.state_dict().values())[2:]\n",
        "    else:\n",
        "        weight_list = list(model.nn_model.state_dict().values()) # The Tensors in this list are of dim : (m_l+1 x m_l)\n",
        "    \n",
        "    weight_array = list(map(lambda x : x.cpu().detach().T.ravel(), weight_list))\n",
        "    weight_array.extend(weight_array)\n",
        "\n",
        "    return torch.cat(weight_array[::2]) # (num_edges x 1) -> We exclude the biases \n",
        "\n",
        "list_of_weights(model, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1474,  0.5293,  0.7033,  0.0385, -0.2693,  0.0281, -0.1351,  0.5443,\n",
              "         0.4106, -0.1474,  0.5293,  0.7033,  0.0385, -0.2693,  0.0281, -0.1351,\n",
              "         0.5443,  0.4106])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQEKDppM6Xad",
        "outputId": "12b83388-7a51-4676-deb4-34d4a0bee7d4"
      },
      "source": [
        "def list_of_importances(importance_scores):\n",
        "    importance_list = []\n",
        "    for array in importance_scores:\n",
        "        importance_list.extend(list(array.ravel()))\n",
        "    \n",
        "    importance_list.extend(importance_list)\n",
        "    return importance_list\n",
        "\n",
        "list_of_importances(compute_importances(model, torch.tensor(X, dtype = torch.float32), device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6965238,\n",
              " 0.8336127,\n",
              " 0.3208684,\n",
              " 0.6123091,\n",
              " 0.8518192,\n",
              " 0.85613644,\n",
              " 0.011083514,\n",
              " 0.21441086,\n",
              " 0.031879663,\n",
              " 0.6965238,\n",
              " 0.8336127,\n",
              " 0.3208684,\n",
              " 0.6123091,\n",
              " 0.8518192,\n",
              " 0.85613644,\n",
              " 0.011083514,\n",
              " 0.21441086,\n",
              " 0.031879663]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kRGBUU_qgSO",
        "outputId": "c45177fb-3a6e-466e-b1ca-3c15acc41d51"
      },
      "source": [
        "def sorting(edges, weights, importances):\n",
        "    sorted_list = [w for e,w,i in sorted(zip(edges, weights, importances), key = lambda x: x[0][0])]\n",
        "    sorted_edges = [e for e,w,i in sorted(zip(edges, weights, importances), key = lambda x: x[0][0])]\n",
        "    sorted_importances = [i for e,w,i in sorted(zip(edges, weights, importances), key = lambda x: x[0][0])]\n",
        "    s_l = [(e,w,i) for e,w,i in sorted(zip(edges, weights, importances), key = lambda x: (x[0][0], x[0][1]))]\n",
        "\n",
        "    edge_attr = [[w,i] for e,w,i in sorted(zip(edges, weights, importances), key = lambda x: (x[0][0], x[0][1]))]\n",
        "    #print(sorted_list)\n",
        "    print(s_l)\n",
        "    #print(sorted_list)\n",
        "    #print(sorted_edges)\n",
        "    print(edge_attr)\n",
        "\n",
        "sorting(unordered_edge_list([2,3,1]), list_of_weights(model, device), list_of_importances(compute_importances(model, torch.tensor(X, dtype = torch.float32), device)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(array([0, 2]), tensor(-0.5079), 0.6965238), (array([0, 3]), tensor(0.6079), 0.8336127), (array([0, 4]), tensor(-0.2340), 0.3208684), (array([1, 2]), tensor(0.4637), 0.6123091), (array([1, 3]), tensor(0.6451), 0.8518192), (array([1, 4]), tensor(0.6483), 0.85613644), (array([2, 0]), tensor(-0.5079), 0.6965238), (array([2, 1]), tensor(0.4637), 0.6123091), (array([2, 5]), tensor(0.4010), 0.011083514), (array([3, 0]), tensor(0.6079), 0.8336127), (array([3, 1]), tensor(0.6451), 0.8518192), (array([3, 5]), tensor(0.1045), 0.21441086), (array([4, 0]), tensor(-0.2340), 0.3208684), (array([4, 1]), tensor(0.6483), 0.85613644), (array([4, 5]), tensor(0.1013), 0.031879663), (array([5, 2]), tensor(0.4010), 0.011083514), (array([5, 3]), tensor(0.1045), 0.21441086), (array([5, 4]), tensor(0.1013), 0.031879663)]\n",
            "[[tensor(-0.5079), 0.6965238], [tensor(0.6079), 0.8336127], [tensor(-0.2340), 0.3208684], [tensor(0.4637), 0.6123091], [tensor(0.6451), 0.8518192], [tensor(0.6483), 0.85613644], [tensor(-0.5079), 0.6965238], [tensor(0.4637), 0.6123091], [tensor(0.4010), 0.011083514], [tensor(0.6079), 0.8336127], [tensor(0.6451), 0.8518192], [tensor(0.1045), 0.21441086], [tensor(-0.2340), 0.3208684], [tensor(0.6483), 0.85613644], [tensor(0.1013), 0.031879663], [tensor(0.4010), 0.011083514], [tensor(0.1045), 0.21441086], [tensor(0.1013), 0.031879663]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ]
}